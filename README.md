Implemented key components of modern large language models (LLMs) from scratch. Designed an end-to-end architecture featuring an embedding layer for token representation, a custom RNN backbone for sequential data processing, and a scaled dot-product attention mechanism to enhance context modeling across timesteps. Created a language modeling head to map hidden states to next-token logits, enabling softmax-based probabilistic distributions for autoregressive text generation. Integrated advanced sampling strategies, including greedy and temperature-based sampling, to support iterative token generation for text completion tasks. Implemented robust training and validation pipelines using cross-entropy loss, optimizing the model over diverse token sequences from the TinyStories dataset to improve generalization and coherence in generated text. The model processes tokenized inputs using pretrained subword tokenization for efficient handling of complex linguistic patterns. Delivered a scalable and modular framework foundational to LLMs like GPT and LLaMA, suitable for text generation and sequence prediction applications.
